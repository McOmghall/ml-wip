%
% A simple LaTeX template for Books
%  (c) Aleksander Morgado <aleksander@es.gnu.org>
%  Released into public domain
%

\documentclass[12pt]{memoir}
\usepackage[a4paper, top=3cm, bottom=3cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage[english,spanish]{babel}
\let\footruleskip\undefined
\usepackage{fancyhdr}
\usepackage[linktocpage=true]{hyperref}
\usepackage{mathptmx}
\usepackage{libertine}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{epigraph}
\usepackage[autostyle]{csquotes} 
\usepackage[style=authoryear,backend=bibtex]{biblatex}
\bibliography{philo}

%Title page command
\newlength\drop
\makeatletter
\newcommand*\titleM{\begingroup% Misericords, T&H p 153
\setlength\drop{0.08\textheight}
\centering
\vspace*{\drop}
{\Huge\bfseries La noción de inteligencia después de \textit{Computing Machinery and Intelligence}}\\[\baselineskip]
{\scshape una perspectiva histórica}\\[\baselineskip]
\vfill
{\large\scshape Pedro Montoto García (USC)}\par
\vfill
{\scshape \@date}\par
\vspace*{2\drop}
\endgroup}
\makeatother

%Keywords command
\providecommand{\keywords}[2]{
	\textbf{\textit{#1: }} #2
}


\begin{document}

\chapterstyle{lyhne}
\pagestyle{empty}
%\pagenumbering{}



% 1st page for the Title
%-------------------------------------------------------------------------------

\begin{titlingpage}
\titleM
\end{titlingpage}

\OnehalfSpacing



\setlength{\epigraphwidth}{0.8\textwidth}
\thispagestyle{empty}
\epigraph{``\textit{Intelligence is} what is measured by intelligence tests.''}{E. Boring, circa 1920}
\newpage


% Not enumerated chapter
%-------------------------------------------------------------------------------
\thispagestyle{empty}
\begin{abstract}
	Este trabajo pretende estudiar la evolución del concepto de inteligencia en los grupos de Inteligencia Artificial a partir de la publicación de \textit{Computing Machinery and Intelligence} \parencite{Turing1950cmi}. Incluímos un comentario crítico de éste artículo y una recensión de los problemas que genera la pregunta \textit{¿Puede pensar una máquina?}, ejemplificados por las críticas academícas a dicho artículo. Se hace un compendio también de los tipos de soluciones que se dan, técnicos y matemáticos, y de los nuevos problemas y conclusiones filosóficas a los que nos lleva ésta. Como guía organizativa se ha usado \citetitle{Nilsson2009} \parencite{Nilsson2009}\footnote{Nils Nilsson es investigador en el \textit{Department of Computer Science} de la Universidad de Stanford, inventor de varios algoritmos de gran importancia en redes neuronales y co-creador del algoritmo $A^*$, que permite descubrir el camino más corto entre dos nodos de un grafo mejorando la búsqueda exhaustiva ponderada (conocida como \textit{algoritmo de Dijkstra}) mediante el uso de métricas heurísticas.}. Como introducción hemos aprovechado para hacer un repaso de los hitos históricos que conducen a la fundación de la Inteligencia Artificial. Además, se ligan los desarrollos tecnológicos de la revolución de los computadores durante la segunda mitad del siglo XX a la disciplina de la IA.
\end{abstract}

\keywords{Palabras clave}{Historia de la Inteligencia Artificial, Filosofía de la Inteligencia Artificial, Alan Turing, Ciencias Cognitivas, Cibernética}

\begin{otherlanguage}{english}
\begin{abstract}
\end{abstract}
\end{otherlanguage}

\keywords{Keywords}{History of Artificial Intelligence, Philosophy of Artificial Intelligence, Alan Turing, Cognitive Sciences, Cybernetics}

\newpage
\thispagestyle{empty}


% If the chapter ends in an odd page, you may want to skip having the page
%  number in the empty page


%Finally, include the ToC
\DoubleSpacing
\begin{KeepFromToc}
  \tableofcontents
\end{KeepFromToc}
\thispagestyle{empty}
\OnehalfSpacing
\newpage

% Define Page style for all chapters
\pagestyle{fancy}
% Delete the current section for header and footer
\fancyhf{}
% Set custom header
\lhead[]{\thepage}
\rhead[\thepage]{}
% Set arabic (1,2,3...) page numbering
\pagenumbering{arabic}

% First enumerated chapter
%-------------------------------------------------------------------------------
\chapter{Introducción Histórica a lo Inteligente y lo Artificial}

\epigraph{``\textit{Intelligence is} a very general mental capability that, among other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience.''}{Common statement with 52 expert signatories, 1997}

Todo lo que llamamos IA comienza con una pregunta de apariencia simple, \textit{¿Es posible construír algo que pueda pensar?}, que en realidad presenta muchas cuestiones asociadas. Podemos citar otras ideas análogas en cierta medida, como crear vida o crear una máquina que resuelva todos los problemas matemáticos. Esta idea no es en absoluto reciente ni mucho menos. La idea de inteligencia artificial ha existido en diversas formas durante la historia del pensamiento occidental, al menos desde la grecia clásica, en mitos, leyendas, historias, especulación y autómatas mecánicos y que, a favor o en contra, intentan dar una respuesta final a esta especie de sueño colectivo.

Las primeras leyendas griegas relevantes de las que tenemos noticia, alrededor del siglo V a.C., de las estatuas de Pigmalión traídas a la vida por Afrodita, diosa de la vida y del amor, y de Hefesto, dios de la forja, construyendo ayudantes dorados para los dioses. De todo esto nos llega noticia a través de la \textit{Política} de Aristóteles, que crea probablemente por accidente uno de los primeros ejemplos de ciencia ficción político-social, planteando la cuestión de qué ocurriría si tuviésemos \textit{máquinas/autómatas/seres artificiales} inteligentes:

\blockquote{For if every instrument could accomplish its own work, obeying or anticipating the will of others, like the statues of Daedalus, or the tripods of Hephaestus, which, says the poet, of their own accord entered the assembly of the Gods; if, in like manner, the shuttle would weave and the plectrum touch the lyre without a hand to guide them, chief workmen would not want servants, nor masters slaves. \parencite{aristotlePolitics}}

Más ejemplos se dan posteriormente. El Talmud, compilado entre los siglos I y VI, habla de \textit{golems} creados de tierra que hombres santos y doctos pueden infundir de vida. En el siglo XII el catalán Ramon Llull inventa lo que el llama \textit{Ars Magna}, un conjunto de discos de papel que convenientemente combinados y rotados permitirían dirimir cualquier discusión teológica mediante el uso de la lógica, con la intención de convertir a las personas de fe musulmana al cristianismo mediante la misma. En algún momento entre finales del siglo XV ó principios del XVI Leonardo da Vinci crea unos esquemas para un robot-caballero que sería supuestamente capaz de sentarse, levantarse y mover los brazos manipulado, eso sí, por un humano\footnote{Éste robot se presentó supuestamente funcionando en una fiesta de la época en la corte de Venecia en 1495 organizada por Ludovico Sforza, y más recientemente un empresario llamado Mark Elling Rosheim reconstruyó los diseños de Leonardo, probando que los mismos eran sensatos y funcionales.}. 

En el siglo XVII se empieza a percibir, en el pensamiento general de la época, la renovación de la corriente filosófica humanista-racional, que se convierte en un cierto consenso social en el cual todo puede llegar a mecanizarse. Se puede decir incluso que hasta ésta época, desde la Grecia clásica, vida e inteligencia sólo podía ser algo otorgado por dioses u otros seres omnipotentes más allá del universo físico, al darse que cualquier creación humana no puede superar una mala imitación de la vida otorgada por la divinidad. Hobbes, en este mismo siglo en su \textit{Leviathan}, contempla la posibilidad de crear un ingenio mecánico que se comporte como un animal, pues todos los órganos para él tienen paralelismos con la mecánica: ``el corazón no es más que un muelle, los nervios son cuerdas'' y conceptos similares aparecen a lo largo de su obra. Descartes, por el contrario, creía que las máquinas serían incapaces de pensamiento real pues sólo están formadas por materia y sería imposible dotarlas de mente. Algunos pensadores creen ver en Leibniz un avance de la Inteligencia Artificial ya que, al igual que Hobbes, concebía que todo lo que hace la mente en última instancia son computaciones, en \textit{De arte combinatoria}. 

En el siglo XVIII Jacques de Vaucanson presenta un autómata que es capaz de simular un pato vivo en ciertos de sus aspectos, construído mediante un armazón metálico adornado con plumas de pato y componentes de relojería y mecánica, que era capaz de comer grano, beber y ``digerir''. En realidad el producto de la digestión ya estaba en el interior del pato para la simulación. Jacques de Vaucanson también es el inventor de las primeras tarjetas perforadas entendidas como contenedoras de programas, secuencias de acciones, para entes mecánicos. Éstas tarjetas se usarán en la ``programación'' de telares mecánicos en el siglo XIX y a principios del siglo XX podremos ver ya máquinas calculadoras que permitían hacer operaciones aritméticas usando estas tarjetas como entrada y salida de información.

También en el siglo XVIII se creó el ``Turco mecánico'', un autómata que podía jugar al ajedrez como un maestro y que como exhibición del mismo fue enviado de gira por las cortes de Europa de la época retando y ganando a soberanos y estrategas. Más tarde se supo que éste ``autómata'' debía su genialidad a un maestro de ajedrez humano que se ponía en su interior en cada partida. Nota bene: La compañía Amazon crea en 2013 un servicio que distribuye y automatiza tareas simples realizables por humanos a voluntarios que cobran por tarea realizada. El nombre del mismo, en homenaje, es Amazon Mechanical Turk y nos permite resaltar que el uso de la inteligencia humana en tareas simples aún no automatizables es, por tanto, una solución bien conocida para las lagunas que la Inteligencia Artificial aún tiene. 

A partir del siglo XIX comenzamos a ver por doquier obras de teatro, narraciones y películas que hablan del \textit{qué ocurriría} si las máquinas pudiesen pensar o actuar como humanos\footnote{Como veremos, del pensar al actuar como humanos hay diferencias que además, se han ido acentuando con el progreso en IA.}. Es evidente, por tanto. que muchos de los problemas de este \textit{posible} que plantea el nacimiento de la IA no son nuevos y sin embargo, el mayor avance que se ha dado en este campo ha sido sin duda alguna los mayores avances se han dado en el siglo XX de la mano de los avances en maquinaria computacional, psicología, neurología y la vertiente artística en la ciencia-ficción.

En última instancia los objetivos de la Inteligencia Artificial son múltiples y variados. Damos una lista de las diversas técnicas que se han creado y describiremos después, compilando y resumiendo desde \cite{mcarthy2007}, ya que sus categorías se adecúan a los experimentos concretos de cada campo pero no generalizan a través de ellos:

\begin{description}
	\item[búsqueda en grafos] Englobamos aquí todos los sistemas cuyo espacio de búsqueda es recursivamente enumerable o generable mediante una función recursiva, aunque también tiene aplicación en subconjuntos recursivamente enumerables de conjuntos no-enumerables. Entran por tanto en esta categoría la resolución de juegos finitos como el ajedrez o la elaboración de planes de acuerdo a restricciones. Razonamiento no monotónico, razonamiento difuso, bots de chat suelen ser representados como grafos difusos (redes de Markov etc.). Podemos incluír también la generación de espacios de soluciones con selección difusa como los algoritmos genéticos, ya que generan un grafo dirigido de orden en el espacio de búsqueda.
\end{description}
\begin{description}
	\item[representación del conocimiento] Todo problema que plantee resolución lógica determinista o no-determinista requiere la representación adecuada del universo del discurso. Hablamos de ontologías y epistemología computacional, e interpretación de lenguajes naturales. Las heurísticas, que son conocimientos a posteriori adquiridos de la experiencia que pueden ser dados al programa o generados por el programa también son representación del conocimiento.
\end{description}
\begin{description}
	\item[pattern matching] Entran en esta categoría las técnicas que permiten resolver problemas de asociación de elementos de un conjunto generalmente grande (imágenes, tomas de muestras de ADN,...) a clases difusas, normalmente mediante redes de neuronas o lógica difusa. La identificación de objetos en imágenes, la clasificación de conjuntos de datos (cuando los puntos de datos son independientes) o segmentos de datos (cuando existe una dependencia entre puntos de datos, como las series temporales) en clases también encaja en esta clase.
\end{description}

Las principales preguntas que se han de responder para responder \textit{¿Es posible construír algo que pueda pensar?} son dos: \textit{¿Qué es pensar?} y \textit{¿Qué es construír?}. Trataremos de ofrecer una visión histórica y panorámica de ambas cuestiones.

\chapter{¿Qué entendemos por construír?\\La base física de la IA}
\epigraph{``\textit{Intelligence is} the ability to use optimally limited resources – including time – to achieve goals.''}{Ray Kurzweil, 2000}
 

Para la construcción de inteligencia precisamos, por tanto, definir qué entendemos por construcción. Esto presenta un número de problemas, como veremos, que no entran por completo en el alcance de este trabajo, pero creemos que conviene reflejar el cambio de concepto que se ha dado en el último siglo, aunque los grupos de tecnólogos y científicos (ingenieros, matemáticos, biólogos, psicólogos y científicos sociales) que trabajan en Inteligencia Artificial no suelan tenerlo en cuenta. 

Normalmente definimos constructo o artefacto\footnote{Para una referencia básica en la filosofía de la tecnología es recomendable consultar \cite{sep-technology} y \cite{sep-artifact}.} como una entidad, que dejaremos como definición genérica pues el \textit{problema del ser} está completamente fuera del alcance del trabajo, en la que algunas o todas sus propiedades preexisten en la intencionalidad de un autor. Es un artefacto si éstas propiedades existen en la descripción de la intencionalidad del autor y/o son aceptadas como válidas en la descripción de lo construído por el autor, i.e.\ el autor determina unas propiedades y valida que el constructo para el que éste ha guiado o accionado el proceso son las que se espera del mismo. Ya desde los tiempos de Aristóteles, en su \textit{Física} se intuye esta definición al decir que los ``productos naturales'' se generan por sus propios impulsos internos mientras que los ``productos artificiales'' precisan de una intencionalidad humana. Avicena criticaba en la edad media que la alquimia jamás podría conseguir ``sustancias genuínas'' como las presentes en la naturaleza precisamente por ser un constructo con intencionalidad humana. En este caso, para mantener el debate cerrado en torno a lo que nos proponemos definir, i.e.\ lo que es un constructo, asumiremos que no existe autor en los entes naturales y que son autogenerados en cierta medida por sí mismos o sus predecesores en el tiempo.

Podemos criticar que la ``vida natural'' no se diferenciaría en absoluto de una supuesta ``vida artificial'' pues la intencionalidad del autor al crear dicha vida vida natural, esto es, las propiedades de dicha forma de vida, debería poseer características en común con la vida ``artificial'', habiendo sólamente cambios en el proceso y los materiales fuente si es una imitación perfecta. Avances en biología, química e Inteligencia Artificial nos llevan en última instancia que lo ``artificial'' podría diferir sólamente de lo ``natural'' en una cuestión de proceso y al no ser posible distinguir lo artificial de lo natural ontológicamente sin analizar los pasos previos que se han dado para obtenerlo podría convertir esta distinción en vacua: i.e.\ si no podemos distinguir sin conocer previamente el proceso, si llegamos a conocimiento del proceso posteriormente no podemos distinguir el proceso ``natural'' del proceso ``artificial''. Las cualidades del objeto que lo hacen ``natural'' o ``artificial'' no son inherentes al objeto una vez la técnica de imitación ha avanzado lo suficiente, como creemos que es la dirección que se está tomando actualmente. Este hecho será de especial relevancia cuando comentemos el juego de la imitación de Turing.

En el contexto de Inteligencia Artificial esto nos lleva, más adelante, al si existe distinción ontológica entre la inteligencia creada artificialmente y la inteligencia natural. Es decir, si logramos crear inteligencia, sea ésta lo que sea, ¿existe aún alguna diferencia entre lo que esta inteligencia es y la inteligencia que nosotros pretendemos? Por ello entramos en una distinción similar a la que propone John Searle (como veremos posteriormente) entre ``IA débil'', significando ``la IA sólo imita la acción de una mente real'', e ``IA fuerte'', significando ``la IA \textit{es} una mente real''. En el argumento que comentamos más arriba las consecuencias son similares, aunque la distinción se haga irrelevante. No debe entenderse este argumento como crítica a Searle pues en el argumento de Searle, que habla sobre el proceso inteligente como constructo versus el proceso inteligente como ente natural, ambos constructos son distinguibles por sus propiedades y mateiales internos, aunque no los observables externamente, por lo que sí existe una posible distinción razonable.

Conviene resaltar que todas las referencias que se han podido compilar para este trabajo no adoptan una postura respecto a las bases filosóficas de la distinción natural-artificial o de la teleología de los componentes de la inteligencia. La mayoría de la literatura se limita a presentar diferentes modelos matemáticos que suelen tomar como base la máquina de Turing, las redes neuronales artificiales, aunque otros llegan a arguír que es necesaria una imitación matemática de los procesos biológicos involucrados en las emociones, normalmente se trata de una lista de tipos de procesos matemáticos, es decir, herramientas para la imitación de la inteligencia según la definición de ésta que se dé.

La convención moderna del constructo computacional, que se instancia físicamente en un soporte tecnológico, y sirve para el soporte físico de la Inteligencia Artificial es la Turing-completitud. No nos detendremos aquí a dar una descripción completa de lo que es la máquina de Turing (o el equivalente cálculo lambda de Church, u otros), sirva decir que es el constructo conceptual en el que se basa la operación de la maquinaria de computación general. En su contexto histórico, 1928, tres años antes de la primera publicación de Gödel sobre el segundo problema de Hilbert y, como continuación del programa formalista, Hilbert elabora una nueva lista de problemas para proponer a la comunidad matemática. Entscheidungsproblem, como se conoce, por su nombre alemán y que en castellano sería ``problema de la decisión'', consiste en dar un procedimiento finito para cualquier fórmula de lógica de primer orden, y por extensión de la aritmética, que nos diga si ésta es verdadera o falsa. Esta idea es heredera directa del décimo problema de Hilbert postulado en 1900, que consistía en similarmente dar un método de pasos finitos para resolver cualquier ecuación diofántica y en una perspectiva histórica de la filosofía leibniziana.

En 1936, cinco años después de On formally undecidable propositions se da un nuevo golpe a este formato de modernismo llamado formalismo o positivismo. Alan Turing publica \textit{On computable numbers, with an application to the Entscheidungsproblem}; en el que formaliza una idea intuitiva de máquina capaz de realizar cálculos \footnote{Que Turing usará en sus implementaciones de las \textit{Bombes}, unas de las primeras máquinas de cálculo eléctricas que fueron usadas para acelerar el descifrado de mensajes alemanes en la Segunda Guerra Mundial}, demostrando que el Entscheidungsproblem es irresoluble mediante estas máquinas; y Alonzo Church publica A note on the Entscheidungsproblem; en el que demuestra que el Entscheidungsproblem es también irresoluble mediante las funciones lambda-definibles (o cálculo lambda) que había estado desarrollando con su estudiante Stephen Kleene. Se considera hoy, dado que Kleene demostró que estas clases de funciones son equivalentes; i.e. toda función lambda-definible puede transformarse mediante un procedimiento finito en una función Turing computable equivalente y viceversa; y que no se ha podido describir ningún algoritmo que no sea describible en estos términos, que esta es la noción formal de computabilidad mediante un procedimiento finito más aproximada a la idea intuitiva de computabilidad.

La máquina de Turing intuitiva, desprovista del aparataje formal matemático\footnote{Sobre el cual puede leerse en detalle en \cite{automata} ó bien en el original \cite{turingComputableNumbers}.}, consiste en una cinta en la que se pueden escribir y sobreescribir símbolos, teniendo capacidad infinita numerable para símbolos y siendo el símbolo la unidad de lectura y escritura, y un dispositivo de máquina de estados que define una función parcial de aplicación continua hasta que se alcanza un estado de parada. Esta función nos dice que si la máquina se encuentra en un estado $x_{t}$ y lee en la cinta un símbolo $s_i$: a) escribirá en esa misma posición un símbolo $s_o$ (que puede ser igual a $s_i$), b) se moverá una posición adelante o atrás, y c) se moverá al estado $x_{t+1}$, o se quedará en $x_{t}$ según el ``programa'' descrito por la función. La definición de función recursiva toma pues, la forma de esta máquina de estados con memoria sobreescribible. 

Pasamos ahora a dar una descripción de los soportes tecnológicos que se han tenido en cuenta para la Inteligencia Artificial, lo que en la jerga del ingeniero en computación sería la infraestructura, y que se compone normalmente de cuatro elementos que establecen abstracciones en el elemento anterior dando lugar a la posibilidad de construír elementos más complejos:

\begin{itemize}
	\item En el núcleo del artefacto tenemos el soporte físico de cómputo: hablamos generalmente de una máquina de cómputo de propósito general con procesador aritmético-lógico y memoria, que contendrá datos y programa de acuerdo a la arquitectura von Neumann, o más recientemente, de varias de éstas máquinas conectadas mediante una red de datos (llamado procesamiento distribuído) o funcionando sobre una memoria compartida (llamado procesamiento paralelo).
	\item Éste elemento se suele cargar con un sistema operativo que establece una capa de abstracción sobre los elementos físicos para que los programadores puedan usar todos los componentes de forma simple y eficiente, ya que se encarga también de otorgar el control de recursos físicos a programas.
	\item Sobre los sistemas operativos se opera mediante lenguajes de programación, que consisten en abstracciones sobre los sistemas del mismo que alcanzan al sistema físico de cómputo y que al mismo tiempo ofrecen una manera de expresar algoritmos. Cada lenguaje ofrece diferentes estructuras computacionales que los hacen más adecuados para expresar un tipo de algoritmo u otro, aunque todos suelen tener la misma capacidad teórica, esto es, la Turing-completitud.
	\item En última instancia tenemos los algoritmos y estructuras de datos que nos permiten resolver el problema que estemos tratando. Ésta es la parte esencial del sistema que identificamos como Inteligencia Artificial, ya que todos los sistemas actuales poseen pequeñas variaciones de los sistemas precedentes siendo los cambios en algoritmia los más relevantes hoy en día.
\end{itemize}

Cada una de estas partes puede considerarse como un problema de ingeniería en sí mismo, lo que ayuda a la simplificación de su resolución en conjunto, y a la división en tareas de los grupos de investigación, puesto que en las últimas décadas se ha pasado de la investigación general en computación a la especialización cada vez más granular en cada uno de estos campos. Por supuesto para llegar a estos elementos desde la concepción simple de las bombes, que eran computadores con un propósito definido y limitado, hubo una evolución de las máquinas de cómputo.

\nocite{wiki:computerhistory}

Históricamente el computador de propósito general fue diseñado por Charles Babbage en 1833, aunque no fue construído hasta el siglo XX por, primero, su hijo Prevost que construyó una parte mínima esencial del mismo que era capaz de ejecutar programas simples en 1910, y en última instancia fue imitado por el Museo de Ciencias de Londres con materiales de la época sin éxito a finales del siglo XX. Aún así se considera que el prototipo, de llegar a cumplir sus especificaciones de diseño, sería Turing-completo, sólo limitado por la precisión numérica y la memoria total de manera similar a la que cualquier computador actual lo sería. En la primera mitad del siglo XX encontramos gran cantidad de máquinas de cómputo, incluso cajeros automáticos de cobro mecánicos y electrónicos con sistemas de ayuda al cálculo de balances, pero ninguna cumple la propiedad de ser lo suficientemente general para ejecutar algoritmos recursivos.

Será en 1944 cuando se cree el Colossus Mark II, el primer computador electrónico digital Turing-completo que se mantuvo en secreto hasta la década de 1970 ya que fue planeado también para romper códigos, y usado durante la guerra fría. Éste constructo es otro ejemplo de la confluencia de ideas que se fue gestando desde el siglo XIX con la publicación de George Boole\footnote{Otros ejemplos de ésta confluencia son los trabajos en lógica de Russell y Whitehead, el proyecto formalista de Hilbert o los teoremas de Gödel.} \textit{Las leyes del pensamiento}, que inaugura el cálculo lógico binario (o booleano), y desarrollado por Alfred Whitehead. Claude Shannon, fundador de la teoría de la información como entropía e ingeniero eléctrico, demuestra que las operaciones de la lógica booleana pueden construírse mediante la adaptación a circuítos eléctricos entendiendo el símbolo básico $0$ como la ausencia de corriente eléctrica y el símbolo $1$ como la presencia de la misma. La idea de usar la lógica simbólica como base de todas las matemáticas, y la idea de \textit{Gödelización} como transformación de enunciados en números y su posterior procesado, confluyen también en la generación de la idea de la Máquina de Turing Universal ya presente en \textit{On Computable Numbers} y consistente en la aplicación de una máquina de Turing que puede aceptar como entrada otras máquinas de Turing transformadas en números naturales y ejecutarlas. Se puede demostrar también que una máquina de Turing, para ser Turing-completa, únicamente precisa del uso de dos símbolos, $0$ y $1$. 

Cabe notar que éste computador era programable en el sentido de que permitía la interconexión arbitraria de elementos que no es que tuviesen programas almacenados sino que eran módulos de cálculo especializados en una tarea concreta referida al cálculo booleano o de aplicación criptográfica, no porque permitiese especificar un programa a nivel de memoria como se hace actualmente, siendo gran parte de estas tareas gestionadas por el sistema operativo. Para la carga en memoria de programas se tendría que esperar a la arquitectura von Neumann y al desarrollo de los chips y memorias magnéticas.

En la década de los 50 todas estas ideas se amplían y se comercializan. La invención de la microprogramación, hoy conocida como \textit{firmware} que permite la adición de nuevas instrucciones programadas sobre una máquina física, sin cambiar la máquina, simplifica el desarrollo de las máquinas de cómputo haciéndolas más flexibles. En 1955 los computadores sustituyen los circuítos basados en tubos de vacío por circuítos basados en transistores, reduciendo su tamaño en órdenes de magnitud y aumentando su velocidad en igual medida. Ésto se suele nombrar en la literatura como ``segunda generación''.

En la década de los 60 se produce el gran salto de la computación al consumo generalizado, no por parte de la inmensa mayoría de la población sino por instituciones y grandes empresas, lanzándose el primer computador que podríamos llamar ``popular'', el IBM 1401. Consistía en un computador configurable que se podía alquilar por unos veinte mil euros mensuales, al cambio actual, lo que permitió a un gran número de entidades entrar en el uso de las computadoras. Lo que hoy conocemos como mainframe requería que un ingeniero configurase el computador y cargase los programas ya escritos en el mismo y preparados para el lenguaje de la máquina que los ejecutaría, decidiendo cuánto tiempo debería gastar cada uno y en qué orden: la automatización de estas tareas dará lugar en las próximas décadas a los sistemas operativos, el uso de mainframes en tiempo compartido y en última instancia a los sistemas distribuídos y paralelos. 

Tambien de esta época son los primeros lenguajes de programación por encima del nivel de lenguaje máquina, LISP y Fortran, que requerían una etapa de traducción intermedia y aún se usan hoy en día. LISP es uno de los lenguajes clave en el desarrollo de la IA en las últimas décadas, ya que, basado en el cálculo lambda de Church, es un lenguaje que permite expresar programas de procesado de listas mediante listas de instrucciones donde datos y programa están expresados en el mismo lenguaje. Los primeros sistemas operativos, como OS/360 cuya historia de desarrollo está narrada en \textit{The mythical Man-Month}, también son de ésta época. En la era del mainframe cada una de las máquinas solía ser enviada al cliente con un sistema operativo adaptado a sus necesidades específicas, los sistemas operativos de consumo general, configurables por sí mismos, no aparecerán hasta la generalización de componentes de consumo que permitan que el sistema operativo pueda ser manufacturado por separado de la máquina, e integrado en varias máquinas.

Los sistemas de tercera y cuarta generación, circuítos integrados conectados mediante cables y circuítos integrados conectados mediante circuítos integrados, serán desarrollados a mediados de la década de los 60 y de la década de los 70 respectivamente. Dado que las diferencias entre ambas generaciones sólo aparecen durante el proceso de manufactura de los mismos hemos decidido obviarlas y centrarnos en los efectos que produce sobre el comercio de computadores: mayor velocidad en menor tamaño y menor precio. Para finales de la década de los 70 el computador personal ya era un hecho y los Apple II, Commodore e IBM-PC podían verse en hogares de todo el mundo, incluída España, donde se considera que hubo una \textit{edad de oro} del software durante los años 80.

Entre los años 80 y nuestra era los computadores han ido incrementando su capacidad computacional en velocidad y capacidad de memoria de acuerdo con la Ley de Moore, que postula que el avance de tecnología permite introducir el doble de transistores en una placa del mismo tamaño más o menos cada dos años. Ésta ley se ha topado recientemente con la limitación práctica de que el calor disipado y energía consumida por un computador altamente integrado también aumentan, precisando que dicho computador posea refrigeración y alimentación eléctrica no razonables. También, teóricamente, se postula un límite duro para dicha ley en el momento que los transistores no puedan funcionar debido a su relación de tamaño con la escala de los fenómenos cuánticos. Por estas razones hoy se tiende a aumentar la eficiencia de los sistemas no aumentando la cantidad de operaciones en serie, sino que se aumenta la cantidad de operaciones que pueden hacerse simultáneamente mediante paralelización o distribución de tareas.

Hoy en día se están empezando a estudiar tecnologías prometedoras como la expansión de la tecnología electrónica mediante elementos ópticos, bajo la promesa de más velocidad de cómputo al ser las interacciones entre fotones más rápidas que las interacciones entre electrones, en la computación cuántica, que utiliza los fenómenos a nivel de partícula atómica para implementar puertas lógicas en una variante de la lógica multivalorada denominada lógica cuántica que permitiría la paralelización masiva de las operaciones atómicas, la computación basada en ADN y biología molecular, que basa su implementación de puertas lógicas en interacciones químicas de proteínas y ácidos o intercambios químicos entre células, como en las sinapsis neuronales. Todo ello nos conduce a pensar que estas tecnologías tienen una relación inherente con la base física o biológica de los seres vivos, y aunque no resulten productivas para los problemas de computación actuales \footnote{Por ejemplo, la computación basada en elementos biológicos resulta más lenta que su equivalente electrónico pero permite el procesado paralelo en una escala mucho mayor, lo que puede ser también una propiedad de la computación cuántica según la implementación que siga} podrían darnos esa capacidad de imitación a todos los niveles que mencionamos al principio del capítulo.

\chapter{La definición de inteligencia}

\epigraph{``[\textit{Intelligence is}] Achieving complex goals in complex environments''}{B. Goertzel, 2006}

La definición de lo que es inteligencia es naturalmente un apartado importante, aunque de difícil solución en el campo de la Filosofía de la Inteligencia Artificial. Se puede ver ver un pequeño compendio de definiciones históricas de inteligencia en diferentes ramas de las ciencias cognitivas en \cite{intDefs}. En primer lugar, trataremos de diferenciar la propiedad ``inteligencia'' que nos parece de un platonismo peligroso al declarar que ésta existe como entidad, pero no es designable lingüísticamente ni en el universo físico, y reemplazarla por ``comportamientos inteligentes'' o ``tareas para las que es necesaria inteligencia''. Se predica normalmente que ser inteligente es un atributo humano, aunque si tomamos ciertas definiciones de inteligencia como válidas podemos atribuír inteligencia a todos los seres vivos o incluso a seres carentes de vida como piedras, que, intuitivamente no parecen tener ninguno de los atributos necesarios para la inteligencia. Es por tanto un tema que merece ser tratado con mucho cuidado.

Entendemos por tanto que hemos de dar una definición de mente o consciencia desde la que trabajaremos, el punto de vista de éste trabajo. Entendemos que la mente al igual que la consciencia es una propiedad emergente de los sistemas físicos que se manifiesta en los comportamientos inteligentes, pero que carece de sentido separada de éstos. Es decir, los comportamientos inteligentes son la base de lo que podemos llamar mente o conciencia, pero no existen comportamientos inteligentes no físicos, de la misma forma que un algoritmo anotado en un papel o cargado en una tarjeta de memoria electrónica no puede generar comportamientos inteligentes a no ser que haya una entidad actuando sobre el mismo. No podemos decir que un cerebro de un animal muerto tenga mente, sin embargo, el cerebro que genera comportamiento en el animal sí podría tenerlo. En última instancia, mente o consciencia es una forma abreviada de referirnos a éstos comportamientos inteligentes.

Comencemos por el que creemos soporte físico de los comportamientos inteligentes en animales y humanos y que da partida probablemente a nuestra pregunta inicial, el cerebro. Se sabe que las señales del sistema nervioso son de naturaleza eléctrica desde los experimentos de Luigi Galvani en el siglo XVIII. La disciplina que estudia la ``maquinaria'' del cerebro, la neurociencia, fue inaugurada en a principios del siglo XX por Camilo Golgi y Santiago Ramón y Cajal. El primero había inventado un método de contraste que permitía distinguir la estructura del tejido cerebral y sus ramificaciones, mientras que el segundo, continuando el uso de éste método creó la teoría de que el tejido cerebral no se componía de una malla de hilos neuronales, sino que dicho tejido estaba formado por células contiguas pero separadas entre sí. 

Las experiencias clínicas de Carl Wernicke y Paul Broca con pacientes con partes del cerebro dañadas, pero que eran capaces de realizar la mayoría de sus funciones vitales, a finales del siglo XIX habían concluído en la teoría de que existían diferentes zonas en el cerebro con diferentes responsabilidades en los procesos de pensamiento y fisiológicos y, por tanto, al ser el daño cerebral localizado sólo estarían afectadas las funciones correspondientes a las zonas dañadas. En 1952 se teoriza el primer modelo de propagación de señales entre neuronas, y se concluye que las neuronas reciben un número de señales de otras neuronas y se activan totalmente o inhiben totalmente las señales recibidas de acuerdo a un umbral interno ($1$ y $0$, de nuevo).

Tenemos por tanto una arquitectura general, los elementos mínimos del sistema estudiados en detalle y además una relación clara con la disciplina de la computación y la comunicación\footnote{La publicación en 1949 de \textit{A mathematical theory of Communication} por Warren Weaver, fundador de la cibernética, y Claude Shannon, que inaugura la cuantificación de la información contenida en un mensaje, es clave para entender esta relación.}, por lo que dada la interpretación mecanicista habitual en la época bastaba estudiar sus relaciones y establecer un modelo matemático para obtener un sistema que nos permitiese conocer y predecir su comportamiento o simulase el objeto de estudio.

Tal y como se dice en \cite{ciberneticsAshby}: ``La cibernética es a la máquina real (electrónica, mecánica, neural o económica) lo que la geometría es a los objetos materiales de nuestro espacio terrestre'', en el sentido de que es una descripción simplificada, y certifica más adelante con ``La cibernética es entonces indiferente al reproche de que algunas de las máquinas que estudia no están incluídas en las que encontramos entre nosotros''. La palabra \textit{Cibernética} se refiere aquí una disciplina generalista creada en la década de 1940 y que dio origen, entre otras cosas, a lo que hoy referimos como Inteligencia Artificial en la ``Conferencia Dartmouth'' de 1956, instituyéndose como una de las ciencias cognitivas como veremos. Descendiente éste término de la misma palabra que dio origen a ``gobierno'' comprendía una descripción general de todos los sistemas complejos, incluídas la vida, las máquinas, la mente humana, la economía y varias disciplinas matemáticas aplicadas intentando encontrar un lenguaje que permitiese expresar las interacciones en todos ellos, es decir ``la ciencia del control'' de todos éstos sistemas. Hoy en día ``cibernética'' se usa en lengauje académico como término para agrupar diferentes campos de éstas ciencias además de en la política, la sociología y los estudios organizativos y de empresa. Parafraseando a \cite{pylyshyn70} en su comentario sobre algoritmia, la cibernética es más una cosmovisión que un campo de la ciencia aislado.

El campo de trabajo de Ashby era, en términos generales, la aplicación de los principios de la ciencia de la computación a la biología y especialmente a la neurociencia. Dejando a un lado que Ashby asume que el comportamiento inteligente es claramente mecánico, entendemos que la Inteligencia Artificial no escapa a la creación de comportamientos inteligentes fuera del ámbito humano pues, en primer lugar, tal vez no interesa la reproducción exacta y completa de todos los comportamientos a los que atribuiríamos inteligencia. Se entiende todo sistema complejo, desde la maquinaria formada por múltiples máquinas simples hasta las redes sociales pasando por los sistemas neuronales como una máquina compleja cuyas leyes son: finitas y cogentes; en el sentido de que unas no contradicen a otras en el mismo sistema, es decir, que el universo descrito no entra en contradicción interna; imitables por otros sistemas de la misma complejidad y sólo dependientes de lo observable externamente.
 
Es decir, en el caso de Ashby aún no se había dado el giro cognitivista respecto a los fenómenos mentales, y Ashby trata la mente como un constructo mecánico cuyas piezas son irrelevantes y de las cuales el funcionamiento observable es lo único relevante. Un funcionalismo que será, 50 años más tarde, destituído en las ciencias cognitivas por dos elementos: en la inteligencia artificial por las técnicas aplicadas a problemas reales, ya que la investigación en la explicación de la mente y el conocimiento será otorgada a los psicólogos, y en la psicología por el cognitivisimo, una corriente que identifica como clave en el comportamiento inteligente a los procesos mentales no observables directamente. 

Veremos a continuación un pequeño resumen de la segunda mitad del siglo XX que explica la convergencia de diversos grupos científicos de lingüistas, psicólogos, neurocientíficos, economistas, filósofos y computólogos en lo que hoy se conoce como \textit{ciencias cognitivas}.

\section{Las ciencias cognitivas y el estudio de la inteligencia}

Definimos \textit{ciencia cognitiva} como toda ciencia que se ocupa de uno o más aspectos de los fenómenos de la cognición. Como tales, las ciencias cognitivas forman un conjunto de ciencias y al mismo tiempo un campo de investigación interdisciplinar cuyo tema central es el estudio de la cognición humana, animal y mecánica \parencite[p.20]{pmf07}, que es a la vez la fuente, se cree, de los comportamientos inteligentes. Se distinguen dos conceptos de cognición: cognición A, aquella que se refiere a la acción de tomar en cuenta una realidad o, dicho de un modo más apropiado a la terminología científica, como recepción de información y cognición B, como el uso y manejo de dicha información. Tanto en el primer y segundo caso se hacen asunciones sobre la capacidad del ser humano de recibir o hacer uso de cualquier información, así como la existencia en cierto grado de dicha información. Existen múltiples concepciones de la información que sirven de base a conocimientos tan diversos como la medición cuantitativa del contenido de un texto fuente (p.e. la concepción informativa de Shannon como desorden) o la fundación de la capacidad deductiva de la
lógica clásica (p.e. la concepción objetivista informativa de Corcoran).

Tal y como lo expresan Allen Newell y Herbert Simon en \textit{Human Problem Solving}
(1972) el ser humano y su capacidad intelectual, y al mismo tiempo las máquinas formales de cómputo (i.e. las máquinas Turing-equivalentes) pertenecen al género, por analogía biológica, `sistema de procesamiento de información'. La constitución de las ciencias cognitivas como grupo de interés en las funciones cognitivas de animales, humanos y máquinas, comienza en la llamada ``conferencia Dartmouth'', una conferencia de matemáticos y lógicos que pone de manifiesto el auge de una disciplina informática llamada inteligencia artificial, cuyo objetivo es la obtención de comportamiento que denominaríamos inteligente en sistemas artificiales: a saber, máquinas sentientes, capaces de actuación o razonamiento autónomo. Nota bene: Más allá de la cognición en animales y máquinas, se comienza a sospechar que los reinos de las plantas, los hongos e incluso los virus tienen algún tipo de cognición. Por ejemplo, se ha demostrado que las plantas segregan neurotransmisores cuando se identifica una situación de estrés (i.e.\ aumento en la acidez del suelo o carencia de agua) y mediante esos neurotransmisores se regula su propio crecimiento \footnote{\url{http://www.sciencedaily.com/releases/2015/07/150729085922.htm} }.

Otro de los momentos importantes para este grupo de interés es el nacimiento de la psicología cognitiva, manifestado en tres hechos: la fundación en 1960 del Harvard Center for Cognitive Studies, la publicación en ese mismo año de \textit{Plans and the structure of Behaviour} y la publicación en 1967 del primer libro de texto de esta escuela de pensamiento psicológico. Con la psicología cognitiva se recupera el mentalismo, esto es, la existencia de una vida mental interna que es al menos parcialmente ajena a la materia y se deja de lado el conductismo, y por tanto el funcionalismo, como mencionamos antes. Además, el postulado de los procesos mentales que es inherente a la psicología cognitiva, asume un cierto compromiso con el computacionalismo, la existencia de procesos que podrían ser simulados o reproducidos mediante máquinas, y acerca esta escuela a los propósitos de la inteligencia artificial.

Aunque inteligencia artificial y psicología cognitiva son nucleares a las ciencias cognitivas no son, evidentemente, las únicas disciplinas que se centran en el estudio de los mecanismos de consciencia e inteligencia que se dan en animales y máquinas. Ciertas disciplinas, como la sociología cognitiva, la pedagogía y la filosofía de la mente, entre otras, trabajan también con este rumbo aportando diferentes perspectivas y técnicas de trabajo. Se pone el énfasis en que las ciencias cognitivas asumen una naturalización materialista de sus postulados, i.e.\ no existen entidades fuera del mundo físico, por tanto la mente, el espíritu y cualquier entidad a la que pueda ser asignada la función creadora de inteligencia debe tener una manifestación física y ningún tipo de atributo no-físico. Por lo tanto, todas las disciplinas que toman parte en las construcciones de las ciencias cognitivas si no asumen esta doctrina metafísica, por así llamarla, deben compatibilizarla con ella\footnote{Es éste el caso de la psicología, cuyo estudio de los procesos mentales obvia en muchos casos los procesos fisiológicos que los desencadenan.}.

\section{Computing Machinery and Intelligence}

El \textit{paper} fundamental que da título a este trabajo e inaugura formalmente la investigación hacia comportamientos inteligentes y vida artificial por parte de los computólogos e ingenieros en computación es \textit{Computing Machinery and Intelligence} de Alan Turing, publicado en 1950. Es éste el primer momento en que puede verse una definición, aunque no formal, sí observable y reproducible de lo que es un comportamiento inteligente. A medida que la Inteligencia Artificial ha ido enfocándose en aplicaciones concretas como la creación de algoritmos para diferentes análisis de datos, el \textit{paper} de Turing ha perdido relevancia en éste campo, mientras que en disciplinas en las que el sujeto de la discusión es propiamente la mente humana como la psicología y la filosofía, éste sigue estando en el punto de mira. Si bien el paper ha perdido relevancia en las ciencias computacionales debido a su enfoque en la resolución de problemas concretos no deja de ser sorprendente lo precisas que son las intuiciones de arquitectura y aumento en capacidad computacional que predice Turing.

En resumidas cuentas, Turing describe la máquina formal de una manera breve e intuitiva y enuncia las reglas del juego que permitiría descubrir si una máquina de éste tipo cargada con cierto programa está pensando o no. Éste juego debe prepararse de tal manera de que una mujer, un hombre y un interrogador que puede hacer preguntas a ambos. Es conveniente que la sala de testeo separe a los sujetos par que no puedan verse entre ellos pero haya canales de comunicación claros, preferentemente mediante texto, para que sea imposible inferir la cualidad humana o no del estilo comunicativo o la voz de cada sujeto. 

Turing plantea la ejecución de tal forma que el interrogador debe adivinar cuál de los dos sujetos es el hombre y cual la mujer, siendo la tarea del hombre convencer al interrogador de que es la mujer y la de la mujer ayudarle a hacer la identificación correcta\footnote{Este hecho ha sido analizado por diversos autores, como David Leavitt en \textit{El hombre que sabía demasiado}, como muestra de sexismo o desconfianza para con las mujeres por parte de Turing. Si bien es un tema interesante, no consideramos que sea relevante en el legado de éste paper, por lo que no nos detendremos en consideraciones al respecto.}. Seguidamente pasa a considerar qué ocurriría si sustituyésemos al hombre por una máquina capaz de realizar conversaciones imitando a humanos. Hay un punto que suele escapar a las discusiones sobre este paper sobre el que nos gustaría llamar la atención y es que Turing explícitamente dice ``Will the interrogator decide wrongly as often when the game is played like this as he does when the game is played between a man and a woman? These questions replace our original, \textit{Can machines think?}'' y es que el juego no trata de identificar siempre a la máquina como humana sino que los errores de identificación de un humano como mujer u hombre sean estadísticamente parecidos a los que se cometan en la identificación como hombre o máquina. Es decir, Turing acepta el error en la identificación y lo hace parte de la prueba ya que es consciente de que una característica fundamental del pensamiento humano es que éste no es exacto y, además, es propenso a errores en múltiples niveles. Existen reformulaciones posteriores de éste test que admiten errores y la posibilidad de que la identificación de comportamiento inteligente se haga por un juicio grupal, como el usado en el premio Loebner\footnote{\url{http://www.aisb.org.uk/events/loebner-prize} }, en el que la identificación humano o no-humano mayoritaria sea la aceptada o bien mediante la aplicación del mismo test varias veces en diferentes momentos. 

Turing pasa a describir las máquinas que podrían ser programadas para participar en el test, que se corresponden con el nivel máquina de la arquitectura de computador digital que describimos en el capítulo anterior, es decir, los niveles de abstracción, como ya hemos explicado, facilitan la implementación del algoritmo proveyendo abstracciones sobre el ``bare metal'', la máquina básica de cómputo, pero no pueden aumentar las capacidades teóricas de la máquina. De la misma manera que al realizar deducciones matemáticas no partimos simplemente del concepto de número y nuestra demostración debe partir de ahí siempre, se toman generalizaciones dadas, por ejemplo, al inferir hechos sobre triángulos se recurre en primera instancia a la trigonometría y no se deduce, de nuevo, la trigonometría de los axiomas de la geometría. Turing indica claramente que lo relevante en éste constructo no es la máquina en sí, sino el algoritmo que produce los comportamientos inteligentes.

Turing comenta una serie de 9 objeciones que impedirían la construcción de dicha máquina, que aquí comentamos desde una perspectiva generalizadora, actualizando las críticas a dichas objeciones:

\begin{description}
	\item[1, 2, 4, 5, 6, 9: La máquina no puede ser humana] Agrupamos 1) Las máquinas carecen de alma, 2) Las consecuencias de la máquina inteligente son temibles ya que el hombre no debe ser sobrepasado, 4) Las máquinas no pueden tener conciencia ni sensibilidad, 5) Las máquinas no pueden X, siendo X una cualidad humana como ``amar, aprender, ser el sujeto de su propio pensamiento, tener errores, etc.'', 6) Ada Lovelace, en sus notas sobre el motor diferencial de Babbage, dice que la máquina ``no puede hacer otra cosa que aquella que sepamos ordenarle cómo hacer, mas no tiene pretensión de generar nada por sí misma'', es decir, carecen de creatividad y 9) Las máquinas no pueden tener Percepción Extra-Sensorial, como equivalentes al basarse todas ellas en la idea de que algo construído ``no puede tener características humanas''. Consideramos 1, 2, 4 y 9 no afectan al test al ser éste funcionalista y las críticas estar fundamentadas en supuestas propiedades de la esencia humana cuya existencia no ha sido probada, o de la estructura interna de la mente. 5 y 6 son dudables ya que también carecen de justificación, basándose en los prejuicios del criticante, siendo ``al estar programadas para el test, deberían ser capaces de simular X'' una contra-crítica equivalente, que es la que de hecho Turing parece esgrimir.
	\item[3, 7: Objeciones matemáticas] Al ser la base de la máquina inteligente propuesta por Turing un constructo capaz de realizar matemática discreta, surgen las siguientes objecciones: 3) La máquina no puede tratar problemas indecidibles de la lógica al estar basada en ella (i.e.\ preguntada por el problema de la parada la máquina daría una respuesta errónea o entraría en un bucle infinito), 7) La máquina es un constructo discreto mientras que el cerebro humano es continuo. Turing considera que no existe demostración de que las limitaciones de 3 no ocurran también en la mente humana y, que si una máquina da una respuesta errónea a estos problemas no es relevante puesto que la mayoría de humanos también la daría. Como respuestas a 7 se dan que los ajustes discretos a funciones continuas podrían dar resultados suficientemente similares para las preguntas formuladas en el juego, y además, como ya hemos visto las neuronas operan fundamentalmente en términos discretos.
	\item[8: El comportamiento humano es arbitrario] Ésta objeción consiste en que no existe un procedimiento finito, algoritmo, para calcular o predecir el comportamiento de un sujeto humano. Se puede contraargumentar que no existe \textit{aún}, como no existe una descripción completa de la física etc.\ o bien se puede argumentar que dado un programa que responde a un número de dieciséis cifras con otro arbitrariamente uno podría similarmente concluír que el programa es humano puesto que exhibe arbitrariedad y no hay reglas claras de su funcionamiento, aunque sabemos que éstas están en su código. Aún así consideramos que es una crítica relevante al funcionamiento del test, puesto que un humano puede identificar a la máquina como humana mientras que otro puede no hacerlo de forma arbitraria.
\end{description}

En la parte final del paper se describe el proceso de aprendizaje máquina, que ha dado un campo propiamente establecido en la inteligencia artificial, y que consiste en hipotetizar que la mente del niño es sustancialmente más simple que la del adulto, siendo la mente del adulto la mente del niño más experiencias de aprendizaje y, esto siendo la clave del constructo, la capacidad de generar ideas secundarias y terciarias a partir de experiencias o ideas inyectadas. Lo que Turing denomina, por analogía con los materiales atómicos, una \textit{mente supercrítica}, en las que la inyección de material nuevo, i.e.\ ideas y experiencias, provoca una reacción que conduce a ideas nuevas: es decir, capacidad creativa y arbitrariedad, de la misma forma que los humanos generarían conceptos nuevos para ellos mismos o incluso para otros humanos. Por la misma analogía, Turing describe las mentes animales como \textit{mentes subcríticas} e incapaces de creatividad o generación de ideas a partir de ideas.

Como crítica decir que el funcionalismo, del que habíamos hablado al citar a Ashby, se da también en la definición de \cite{Turing1950cmi}, ya que identifica comportamiento inteligente como ``comportamiento percibido como humano por un ser humano'', además de ser un funcionalismo subjetivo en el que la validez de la observación es dependiente de un agente o varios, según la formulación del test, i.e.\ no es una métrica objetiva. Además, ésta definición es limitante, pues sólo identifica comportamientos inteligentes o pensamientos con actuaciones humanas, siendo ambas identificaciones comportamiento-pensamiento y humano-inteligente problemáticas: la identificación comportamiento-pensamiento impide a la IA dar una descripción de lo que es pensamiento más allá de ``es comportamiento'' y la identificación humano-inteligente impide la resolución de problemas que están más allá del proceso de pensamiento humano en formas diferentes que un simple aumento en la velocidad o precisión de los mismos. 

Ésta crítica al funcionalismo también está implícita en el contra-argumento de la habitación china, por John Searle.

\section{La habitación china}

% Last pages for ToC
%-------------------------------------------------------------------------------
\newpage

\printbibliography

\newpage


\epigraph{Grand Master Turing once dreamed that he was a machine. When he awoke he exclaimed: \\

“I don't know whether I am Turing dreaming that I am a machine, or a machine dreaming that I am Turing!”}{From the Tao of Computer Programming}

\end{document}

