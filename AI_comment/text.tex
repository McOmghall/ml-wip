%
% A simple LaTeX template for Books
%  (c) Aleksander Morgado <aleksander@es.gnu.org>
%  Released into public domain
%

\documentclass[12pt]{memoir}
\usepackage[a4paper, top=3cm, bottom=3cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage[spanish]{babel}
\let\footruleskip\undefined
\usepackage{fancyhdr}
\usepackage[linktocpage=true]{hyperref}
\usepackage{mathptmx}
\usepackage{libertine}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{epigraph}
\usepackage[autostyle]{csquotes}  
\usepackage[style=authoryear,backend=bibtex]{biblatex}
\bibliography{philo}

%Title page command
\newlength\drop
\makeatletter
\newcommand*\titleM{\begingroup% Misericords, T&H p 153
\setlength\drop{0.08\textheight}
\centering
\vspace*{\drop}
{\Huge\bfseries La noción de inteligencia después de \textit{Computing Machinery and Intelligence}}\\[\baselineskip]
{\scshape una perspectiva histórica}\\[\baselineskip]
\vfill
{\large\scshape Pedro Montoto García (USC)}\par
\vfill
{\scshape \@date}\par
\vspace*{2\drop}
\endgroup}
\makeatother

%Keywords command
\providecommand{\keywords}[2]{
	\textbf{\textit{Keywords: }} #1 \\
	\textbf{\textit{Palabras clave: }} #2
}


\begin{document}

\chapterstyle{lyhne}
\pagestyle{empty}
%\pagenumbering{}



% 1st page for the Title
%-------------------------------------------------------------------------------

\begin{titlingpage}
\titleM
\end{titlingpage}



\setlength{\epigraphwidth}{0.8\textwidth}
\thispagestyle{empty}
\epigraph{``\textit{Intelligence is} what is measured by intelligence tests.''}{E. Boring, circa 1920}
\newpage


% Not enumerated chapter
%-------------------------------------------------------------------------------
\thispagestyle{empty}
\begin{abstract}
	Este trabajo pretende estudiar la evolución del concepto de inteligencia\footnote{Para ver una pequeña lista de definiciones históricas de inteligencia en diferentes ramas de las ciencias cognitivas ver \cite{intDefs}.} en los grupos de Inteligencia Artificial a partir de la publicación por parte de Alan Turing de \textit{Computing Machinery and Intelligence} en \cite{Turing1950cmi}, las impresiones y técnicas generadas a partir de éste artículo y otros por los investigadores en esta disciplina en la década de 1960 que han tenido un impacto en la vertiente filosófica de este problema y en última instancia proponer una clasificación de los tipos de problemas que se intentan resolver en la disciplina hoy en día contrastándolos con las intenciones de los fundadores de la disciplina en la década de los 60. Se hará una recensión de los problemas que la pregunta \textbf{¿Puede pensar una máquina?} genera, de los tipos de soluciones técnicas que se dan con los problemas que éstas enfrentan, técnicos y matemáticos, y de los nuevos problemas y conclusiones filosóficas a los que nos lleva ésta. Como guía organizativa se ha usado el trabajo de Nils Nilsson\footnote{Investigador en el \textit{Department of Computer Science} de la Universidad de Stanford, inventor de varios algoritmos de gran importancia en redes neuronales y co-creador del algoritmo $A^*$, que permite descubrir el mejor camino más corto entre dos nodos de un grafo mejorando la búsqueda exhaustiva ponderada (conocida como \textit{algoritmo de Dijkstra}) mediante el uso de métricas heurísticas.} \parencite{Nilsson2009} sobre historia de la IA titulado \citetitle{Nilsson2009}.
\end{abstract}
\keywords{Turing, artificial intelligence, history, algorithms, cognitive psychology, cybernetics}{Turing, inteligencia artificial, historia, algoritmos, psicología cognitiva, cibernética}

\newpage
\thispagestyle{empty}


% Set double spacing for the text
\DoubleSpacing

% If the chapter ends in an odd page, you may want to skip having the page
%  number in the empty page


%Finally, include the ToC
\begin{KeepFromToc}
  \tableofcontents
\end{KeepFromToc}
\thispagestyle{empty}
\newpage

% Define Page style for all chapters
\pagestyle{fancy}
% Delete the current section for header and footer
\fancyhf{}
% Set custom header
\lhead[]{\thepage}
\rhead[\thepage]{}
% Set arabic (1,2,3...) page numbering
\pagenumbering{arabic}

% First enumerated chapter
%-------------------------------------------------------------------------------
\chapter{Introducción Histórica a lo Inteligente y lo Artificial}

\epigraph{``\textit{Intelligence is} a very general mental capability that, among other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly and learn from experience.''}{Common statement with 52 expert signatories, 1997}

Todo lo que llamamos IA comienza con una pregunta de apariencia simple, \textit{¿Es posible construír algo que pueda pensar?}, que en realidad presenta muchas cuestiones asociadas. Podemos citar otras ideas análogas en cierta medida, como crear vida o crear una máquina que resuelva todos los problemas matemáticos. Esta idea no es en absoluto reciente ni mucho menos, ya que existen leyendas griegas clásicas alrededor del siglo V antes de Cristo bien conocidas de estatuas de Pigmalión traídas a la vida por Afrodita, diosa de la vida y del amor, y de Hefesto, dios de la forja, construyendo ayudantes dorados para los dioses. De todo esto nos llega noticia a través de la \textit{Política} de Aristóteles, que crea probablemente por accidente uno de los primeros ejemplos de ciencia ficción político-social, planteando la cuestión de qué ocurriría si tuviésemos \textit{máquinas/autómatas/seres artificiales} inteligentes:

\blockquote{For if every instrument could accomplish its own work, obeying or anticipating the will of others, like the statues of Daedalus, or the tripods of Hephaestus, which, says the poet, of their own accord entered the assembly of the Gods; if, in like manner, the shuttle would weave and the plectrum touch the lyre without a hand to guide them, \textbf{chief workmen would not want servants, nor masters slaves}. Here, however, another distinction must be drawn; the instruments commonly so called are instruments of production, whilst a possession is an instrument of action. \parencite{aristotlePolitics}}

Más ejemplos se dan posteriormente. El Talmud, compilado entre los siglos I y VI, habla de \textit{golems} creados de tierra que hombres santos y doctos pueden infundir de vida. En el siglo XII el catalán Ramon Llull inventa lo que el llama \textit{Ars Magna}, un conjunto de discos de papel que convenientemente combinados y rotados permitirían dirimir cualquier discusión teológica mediante el uso de la lógica, con la intención de convertir a las personas de fe musulmana al cristianismo mediante la misma. En algún momento entre finales del siglo XV ó principios del XVI Leonardo da Vinci crea unos esquemas para un robot-caballero que sería supuestamente capaz de sentarse, levantarse y mover los brazos manipulado, eso sí, por un humano\footnote{Éste robot se presentó supuestamente funcionando en una fiesta de la época en la corte de Venecia en 1495 organizada por Ludovico Sforza, y más recientemente un empresario llamado Mark Elling Rosheim reconstruyó los diseños de Leonardo, probando que los mismos eran sensatos y funcionales.}. 

En el siglo XVII se empieza a percibir, en el pensamiento general de la época, la renovación de la corriente filosófica humanista-racional, que se convierte en un cierto consenso social en el cual todo puede llegar a mecanizarse. Se puede decir incluso que hasta ésta época, desde la Grecia clásica, vida e inteligencia sólo podía ser algo otorgado por dioses u otros seres omnipotentes más allá del universo físico, al darse que cualquier creación humana no puede superar una mala imitación de la vida otorgada por la divinidad. Hobbes, en este mismo siglo en su \textit{Leviathan}, contempla la posibilidad de crear un ingenio mecánico que se comporte como un animal, pues todos los órganos para él tienen paralelismos con la mecánica: ``el corazón no es más que un muelle, los nervios son cuerdas'' y conceptos similares aparecen a lo largo de su obra. 

En el siglo XVIII Jacques de Vaucanson presenta un autómata que es capaz de simular un pato vivo en ciertos de sus aspectos, construído mediante un armazón metálico adornado con plumas de pato y componentes de relojería y mecánica, que era capaz de comer grano, beber y ``digerir''. En realidad el producto de la digestión ya estaba en el interior del pato para la simulación. Jacques de Vaucanson también es el inventor de las primeras tarjetas perforadas entendidas como contenedoras de programas, secuencias de acciones, para entes mecánicos. Éstas tarjetas se usarán en la ``programación'' de telares mecánicos en el siglo XIX y a principios del siglo XX podremos ver ya máquinas calculadoras que permitían hacer operaciones aritméticas usando estas tarjetas como entrada y salida de información.

También en el siglo XVIII se creó el ``Turco mecánico'', un autómata que podía jugar al ajedrez como un maestro y que como exhibición del mismo fue enviado de gira por las cortes de Europa de la época retando y ganando a soberanos y estrategas. Más tarde se supo que éste ``autómata'' debía su genialidad a un maestro de ajedrez humano que se ponía en su interior en cada partida. Nota bene: La compañía Amazon crea en 2013 un servicio que distribuye y automatiza tareas simples realizables por humanos a voluntarios que cobran por tarea realizada. El nombre del mismo, en homenaje, es Amazon Mechanical Turk y nos permite resaltar que el uso de la inteligencia humana en tareas simples aún no automatizables es, por tanto, una solución bien conocida para las lagunas que la Inteligencia Artificial aún tiene. 

A partir del siglo XIX comenzamos a ver por doquier obras de teatro, narraciones y películas que hablan del \textit{qué ocurriría} si las máquinas pudiesen pensar o actuar como humanos\footnote{Como veremos, del pensar al actuar como humanos hay diferencias que además, se han ido acentuando con el progreso en IA.}. Es evidente, por tanto. que muchos de los problemas de este \textit{posible} que plantea el nacimiento de la IA no son nuevos y sin embargo, el mayor avance que se ha dado en este campo ha sido sin duda alguna los mayores avances se han dado en el siglo XX de la mano de los avances en maquinaria computacional, psicología, neurología y la vertiente artística en la ciencia-ficción.

En última instancia los objetivos de la Inteligencia Artificial son múltiples y variados. Damos una lista de las diversas técnicas que se han creado y describiremos después, compilando y resumiendo desde \cite{mcarthy2007}, ya que sus categorías se adecúan a los experimentos concretos de cada campo pero no generalizan a través de ellos:

\begin{description}
	\item[búsqueda en grafos] Englobamos aquí todos los sistemas cuyo espacio de búsqueda es recursivamente enumerable o generable mediante una función recursiva, aunque también tiene aplicación en subconjuntos recursivamente enumerables de conjuntos no-enumerables. Entran por tanto en esta categoría la resolución de juegos finitos como el ajedrez o la elaboración de planes de acuerdo a restricciones. Razonamiento no monotónico, razonamiento difuso, bots de chat suelen ser representados como grafos difusos (redes de Markov etc.). Podemos incluír también la generación de espacios de soluciones con selección difusa como los algoritmos genéticos, ya que generan un grafo dirigido de orden en el espacio de búsqueda.
\end{description}
\begin{description}
	\item[representación del conocimiento] Todo problema que plantee resolución lógica determinista o no-determinista requiere la representación adecuada del universo del discurso. Hablamos de ontologías y epistemología computacional, e interpretación de lenguajes naturales. Las heurísticas, que son conocimientos a posteriori adquiridos de la experiencia que pueden ser dados al programa o generados por el programa también son representación del conocimiento.
\end{description}
\begin{description}
	\item[pattern matching] Entran en esta categoría las técnicas que permiten resolver problemas de asociación de elementos de un conjunto generalmente grande (imágenes, tomas de muestras de ADN,...) a clases difusas, normalmente mediante redes de neuronas o lógica difusa. La identificación de objetos en imágenes, la clasificación de conjuntos de datos (cuando los puntos de datos son independientes) o segmentos de datos (cuando existe una dependencia entre puntos de datos, como las series temporales) en clases también encaja en esta clase.
\end{description}

Las principales preguntas que se han de responder para responder \textit{¿Es posible construír algo que pueda pensar?} son dos: \textit{¿Qué es pensar?} y \textit{¿Qué es construír?}. Trataremos de ofrecer una visión histórica y panorámica de ambas cuestiones.

\chapter{¿Qué entendemos por construír?\\La base física de la IA}
\epigraph{``\textit{Intelligence is} the ability to use optimally limited resources – including time – to achieve goals.''}{Ray Kurzweil, 2000}
 

Para la construcción de inteligencia precisamos, por tanto, definir qué entendemos por construcción. Esto presenta un número de problemas, como veremos, que no entran por completo en el alcance de este trabajo, pero creemos que conviene reflejar el cambio de concepto que se ha dado en el último siglo, aunque los grupos de tecnólogos y científicos (ingenieros, matemáticos, biólogos, psicólogos y científicos sociales) que trabajan en Inteligencia Artificial no suelan tenerlo en cuenta. 

Normalmente definimos constructo o artefacto\footnote{Para una referencia básica en la filosofía de la tecnología es recomendable consultar \cite{sep-technology} y \cite{sep-artifact}.} como una entidad, que dejaremos como definición genérica pues el \textit{problema del ser} está completamente fuera del alcance del trabajo, en la que algunas o todas sus propiedades preexisten en la intencionalidad de un autor. Es un artefacto si éstas propiedades existen en la descripción de la intencionalidad del autor y/o son aceptadas como válidas en la descripción de lo construído por el autor, i.e.\ el autor determina unas propiedades y valida que el constructo para el que éste ha guiado o accionado el proceso son las que se espera del mismo. Ya desde los tiempos de Aristóteles, en su \textit{Física} se intuye esta definición al decir que los ``productos naturales'' se generan por sus propios impulsos internos mientras que los ``productos artificiales'' precisan de una intencionalidad humana. Avicena criticaba en la edad media que la alquimia jamás podría conseguir ``sustancias genuínas'' como las presentes en la naturaleza precisamente por ser un constructo con intencionalidad humana. En este caso, para mantener el debate cerrado en torno a lo que nos proponemos definir, i.e.\ lo que es un constructo, asumiremos que no existe autor en los entes naturales y que son autogenerados en cierta medida por sí mismos o sus predecesores en el tiempo.

Podemos criticar que la ``vida natural'' no se diferenciaría en absoluto de una supuesta ``vida artificial'' pues la intencionalidad del autor al crear dicha vida vida natural, esto es, las propiedades de dicha forma de vida, debería poseer características en común con la vida ``artificial'', habiendo sólamente cambios en el proceso y los materiales fuente si es una imitación perfecta. Avances en biología, química e Inteligencia Artificial nos llevan en última instancia que lo ``artificial'' podría diferir sólamente de lo ``natural'' en una cuestión de proceso y al no ser posible distinguir lo artificial de lo natural ontológicamente sin analizar los pasos previos que se han dado para obtenerlo podría convertir esta distinción en vacua: i.e.\ si no podemos distinguir sin conocer previamente el proceso, si llegamos a conocimiento del proceso posteriormente no podemos distinguir el proceso ``natural'' del proceso ``artificial''. Las cualidades del objeto que lo hacen ``natural'' o ``artificial'' no son inherentes al objeto una vez la técnica de imitación ha avanzado lo suficiente, como creemos que es la dirección que se está tomando actualmente. Este hecho será de especial relevancia cuando comentemos el juego de la imitación de Turing.

En el contexto de Inteligencia Artificial esto nos lleva, más adelante, al si existe distinción ontológica entre la inteligencia creada artificialmente y la inteligencia natural. Es decir, si logramos crear inteligencia, sea ésta lo que sea, ¿existe aún alguna diferencia entre lo que esta inteligencia es y la inteligencia que nosotros pretendemos? Por ello entramos en una distinción similar a la que propone John Searle entre ``IA débil'', significando ``la IA sólo imita la acción de una mente real'', e ``IA fuerte'', significando ``la IA \textit{es} una mente real''. En el argumento que comentamos más arriba las consecuencias son similares, aunque la distinción se haga irrelevante. No debe entenderse este argumento como crítica a Searle pues en el argumento de Searle, que habla sobre el proceso inteligente como constructo versus el proceso inteligente como ente natural, ambos constructos son distinguibles por sus propiedades y mateiales internos, aunque no los observables externamente, por lo que sí existe una posible distinción razonable.

Conviene resaltar que todas las referencias que se han podido compilar para este trabajo no adoptan una postura respecto a las bases filosóficas de la distinción natural-artificial o de la teleología de los componentes de la inteligencia. La mayoría de la literatura se limita a presentar diferentes modelos matemáticos que suelen tomar como base la máquina de Turing, las redes neuronales artificiales, aunque otros llegan a arguír que es necesaria una imitación matemática de los procesos biológicos involucrados en las emociones, normalmente se trata de una lista de tipos de procesos matemáticos, es decir, herramientas para la imitación de la inteligencia según la definición de ésta que se dé.

La convención moderna del constructo computacional, que se instancia físicamente en un soporte tecnológico, y sirve para el soporte físico de la Inteligencia Artificial es la Turing-completitud. No nos detendremos aquí a dar una descripción completa de lo que es la máquina de Turing (o el equivalente cálculo lambda de Church, u otros), sirva decir que es el constructo conceptual en el que se basa la operación de la maquinaria de computación general. En su contexto histórico, 1928, tres años antes de la primera publicación de Gödel sobre el segundo problema de Hilbert y, como continuación del programa formalista, Hilbert elabora una nueva lista de problemas para proponer a la comunidad matemática. Entscheidungsproblem, como se conoce, por su nombre alemán y que en castellano sería ``problema de la decisión'', consiste en dar un procedimiento finito para cualquier fórmula de lógica de primer orden, y por extensión de la aritmética, que nos diga si ésta es verdadera o falsa. Esta idea es heredera directa del décimo problema de Hilbert postulado en 1900, que consistía en similarmente dar un método de pasos finitos para resolver cualquier ecuación diofántica y en una perspectiva histórica de la filosofía leibniziana.

En 1936, cinco años después de On formally undecidable propositions se da un nuevo golpe a este formato de modernismo llamado formalismo o positivismo. Alan Turing publica \textit{On computable numbers, with an application to the Entscheidungsproblem}; en el que formaliza una idea intuitiva de máquina capaz de realizar cálculos \footnote{Que Turing usará en sus implementaciones de las \textit{Bombes}, unas de las primeras máquinas de cálculo eléctricas que fueron usadas para acelerar el descifrado de mensajes alemanes en la Segunda Guerra Mundial}, demostrando que el Entscheidungsproblem es irresoluble mediante estas máquinas; y Alonzo Church publica A note on the Entscheidungsproblem; en el que demuestra que el Entscheidungsproblem es también irresoluble mediante las funciones lambda-definibles (o cálculo lambda) que había estado desarrollando con su estudiante Stephen Kleene. Se considera hoy, dado que Kleene demostró que estas clases de funciones son equivalentes; i.e. toda función lambda-definible puede transformarse mediante un procedimiento finito en una función Turing computable equivalente y viceversa; y que no se ha podido describir ningún algoritmo que no sea describible en estos términos, que esta es la noción formal de computabilidad mediante un procedimiento finito más aproximada a la idea intuitiva de computabilidad.

La máquina de Turing intuitiva, desprovista del aparataje formal matemático\footnote{Sobre el cual puede leerse en detalle en \cite{automata} ó bien en el original \cite{turingComputableNumbers}.}, consiste en una cinta en la que se pueden escribir y sobreescribir símbolos, teniendo capacidad infinita numerable para símbolos y siendo el símbolo la unidad de lectura y escritura, y un dispositivo de máquina de estados que define una función parcial de aplicación continua hasta que se alcanza un estado de parada. Esta función nos dice que si la máquina se encuentra en un estado $x_{t}$ y lee en la cinta un símbolo $s_i$: a) escribirá en esa misma posición un símbolo $s_o$ (que puede ser igual a $s_i$), b) se moverá una posición adelante o atrás, y c) se moverá al estado $x_{t+1}$, o se quedará en $x_{t}$ según el ``programa'' descrito por la función. La definición de función recursiva toma pues, la forma de esta máquina de estados con memoria sobreescribible. 

Pasamos ahora a dar una descripción de los soportes tecnológicos que se han tenido en cuenta para la Inteligencia Artificial, lo que en la jerga del ingeniero en computación sería la infraestructura, y que se compone normalmente de cuatro elementos que establecen abstracciones en el elemento anterior dando lugar a la posibilidad de construír elementos más complejos:

\begin{itemize}
	\item En el núcleo del artefacto tenemos el soporte físico de cómputo: hablamos generalmente de una máquina de cómputo de propósito general con procesador aritmético-lógico y memoria, que contendrá datos y programa de acuerdo a la arquitectura von Neumann, o más recientemente, de varias de éstas máquinas conectadas mediante una red de datos (llamado procesamiento distribuído) o funcionando sobre una memoria compartida (llamado procesamiento paralelo).
	\item Éste elemento se suele cargar con un sistema operativo que establece una capa de abstracción sobre los elementos físicos para que los programadores puedan usar todos los componentes de forma simple y eficiente, ya que se encarga también de otorgar el control de recursos físicos a programas.
	\item Sobre los sistemas operativos se opera mediante lenguajes de programación, que consisten en abstracciones sobre los sistemas del mismo que alcanzan al sistema físico de cómputo y que al mismo tiempo ofrecen una manera de expresar algoritmos. Cada lenguaje ofrece diferentes estructuras computacionales que los hacen más adecuados para expresar un tipo de algoritmo u otro, aunque todos suelen tener la misma capacidad teórica, esto es, la Turing-completitud.
	\item En última instancia tenemos los algoritmos y estructuras de datos que nos permiten resolver el problema que estemos tratando. Ésta es la parte esencial del sistema que identificamos como Inteligencia Artificial, ya que todos los sistemas actuales poseen pequeñas variaciones de los sistemas precedentes siendo los cambios en algoritmia los más relevantes hoy en día.
\end{itemize}

Cada una de estas partes puede considerarse como un problema de ingeniería en sí mismo, lo que ayuda a la simplificación de su resolución en conjunto, y a la división en tareas de los grupos de investigación, puesto que en las últimas décadas se ha pasado de la investigación general en computación a la especialización cada vez más granular en cada uno de estos campos. Por supuesto para llegar a estos elementos desde la concepción simple de las bombes, que eran computadores con un propósito definido y limitado, hubo una evolución de las máquinas de cómputo.

\nocite{wiki:computerhistory}

Históricamente el computador de propósito general fue diseñado por Charles Babbage en 1833, aunque no fue construído hasta el siglo XX por, primero, su hijo Prevost que construyó una parte mínima esencial del mismo que era capaz de ejecutar programas simples en 1910, y en última instancia fue imitado por el Museo de Ciencias de Londres con materiales de la época sin éxito a finales del siglo XX. Aún así se considera que el prototipo, de llegar a cumplir sus especificaciones de diseño, sería Turing-completo, sólo limitado por la precisión numérica y la memoria total de manera similar a la que cualquier computador actual lo sería. En la primera mitad del siglo XX encontramos gran cantidad de máquinas de cómputo, incluso cajeros automáticos de cobro mecánicos y electrónicos con sistemas de ayuda al cálculo de balances, pero ninguna cumple la propiedad de ser lo suficientemente general para ejecutar algoritmos recursivos.

Será en 1944 cuando se cree el Colossus Mark II, el primer computador electrónico digital Turing-completo que se mantuvo en secreto hasta la década de 1970 ya que fue planeado también para romper códigos, y usado durante la guerra fría. Éste constructo es otro ejemplo de la confluencia de ideas que se fue gestando desde el siglo XIX con la publicación de George Boole\footnote{Otros ejemplos de ésta confluencia son los trabajos en lógica de Russell y Whitehead, el proyecto formalista de Hilbert o los teoremas de Gödel.} \textit{Las leyes del pensamiento}, que inaugura el cálculo lógico binario (o booleano), y desarrollado por Alfred Whitehead. Claude Shannon, fundador de la teoría de la información como entropía e ingeniero eléctrico, demuestra que las operaciones de la lógica booleana pueden construírse mediante la adaptación a circuítos eléctricos entendiendo el símbolo básico $0$ como la ausencia de corriente eléctrica y el símbolo $1$ como la presencia de la misma. La idea de usar la lógica simbólica como base de todas las matemáticas, y la idea de \textit{Gödelización} como transformación de enunciados en números y su posterior procesado, confluyen también en la generación de la idea de la Máquina de Turing Universal ya presente en \textit{On Computable Numbers} y consistente en la aplicación de una máquina de Turing que puede aceptar como entrada otras máquinas de Turing transformadas en números naturales y ejecutarlas. Se puede demostrar también que una máquina de Turing, para ser Turing-completa, únicamente precisa del uso de dos símbolos, $0$ y $1$. 

Cabe notar que éste computador era programable en el sentido de que permitía la interconexión arbitraria de elementos que no es que tuviesen programas almacenados sino que eran módulos de cálculo especializados en una tarea concreta referida al cálculo booleano o de aplicación criptográfica, no porque permitiese especificar un programa a nivel de memoria como se hace actualmente, siendo gran parte de estas tareas gestionadas por el sistema operativo. Para la carga en memoria de programas se tendría que esperar a la arquitectura von Neumann y al desarrollo de los chips y memorias magnéticas.

En la década de los 50 todas estas ideas se amplían y se comercializan. La invención de la microprogramación, hoy conocida como \textit{firmware} que permite la adición de nuevas instrucciones programadas sobre una máquina física, sin cambiar la máquina, simplifica el desarrollo de las máquinas de cómputo haciéndolas más flexibles. En 1955 los computadores sustituyen los circuítos basados en tubos de vacío por circuítos basados en transistores, reduciendo su tamaño en órdenes de magnitud y aumentando su velocidad en igual medida. Ésto se suele nombrar en la literatura como ``segunda generación''.

En la década de los 60 se produce el gran salto de la computación al consumo generalizado, no por parte de la inmensa mayoría de la población sino por instituciones y grandes empresas, lanzándose el primer computador que podríamos llamar ``popular'', el IBM 1401. Consistía en un computador configurable que se podía alquilar por unos veinte mil euros mensuales, al cambio actual, lo que permitió a un gran número de entidades entrar en el uso de las computadoras. Lo que hoy conocemos como mainframe requería que un ingeniero configurase el computador y cargase los programas ya escritos en el mismo y preparados para el lenguaje de la máquina que los ejecutaría, decidiendo cuánto tiempo debería gastar cada uno y en qué orden: la automatización de estas tareas dará lugar en las próximas décadas a los sistemas operativos, el uso de mainframes en tiempo compartido y en última instancia a los sistemas distribuídos y paralelos. 

Tambien de esta época son los primeros lenguajes de programación por encima del nivel de lenguaje máquina, LISP y Fortran, que requerían una etapa de traducción intermedia y aún se usan hoy en día. LISP es uno de los lenguajes clave en el desarrollo de la IA en las últimas décadas, ya que, basado en el cálculo lambda de Church, es un lenguaje que permite expresar programas de procesado de listas mediante listas de instrucciones donde datos y programa están expresados en el mismo lenguaje. Los primeros sistemas operativos, como OS/360 cuya historia de desarrollo está narrada en \textit{The mythical Man-Month}, también son de ésta época. En la era del mainframe cada una de las máquinas solía ser enviada al cliente con un sistema operativo adaptado a sus necesidades específicas, los sistemas operativos de consumo general, configurables por sí mismos, no aparecerán hasta la generalización de componentes de consumo que permitan que el sistema operativo pueda ser manufacturado por separado de la máquina, e integrado en varias máquinas.

Los sistemas de tercera y cuarta generación, circuítos integrados conectados mediante cables y circuítos integrados conectados mediante circuítos integrados, serán desarrollados a mediados de la década de los 60 y de la década de los 70 respectivamente. Dado que las diferencias entre ambas generaciones sólo aparecen durante el proceso de manufactura de los mismos hemos decidido obviarlas y centrarnos en los efectos que produce sobre el comercio de computadores: mayor velocidad en menor tamaño y menor precio. Para finales de la década de los 70 el computador personal ya era un hecho y los Apple II, Commodore e IBM-PC podían verse en hogares de todo el mundo, incluída España, donde se considera que hubo una \textit{edad de oro} del software durante los años 80.

Entre los años 80 y nuestra era los computadores han ido incrementando su capacidad computacional en velocidad y capacidad de memoria de acuerdo con la Ley de Moore, que postula que el avance de tecnología permite introducir el doble de transistores en una placa del mismo tamaño más o menos cada dos años. Ésta ley se ha topado recientemente con la limitación práctica de que el calor disipado y energía consumida por un computador altamente integrado también aumentan, precisando que dicho computador posea refrigeración y alimentación eléctrica no razonables. También, teóricamente, se postula un límite duro para dicha ley en el momento que los transistores no puedan funcionar debido a su relación de tamaño con la escala de los fenómenos cuánticos. Por estas razones hoy se tiende a aumentar la eficiencia de los sistemas no aumentando la cantidad de operaciones en serie, sino que se aumenta la cantidad de operaciones que pueden hacerse simultáneamente mediante paralelización o distribución de tareas.

En el futuro existen tecnologías prometedoras en la expansión de la tecnología electrónica mediante elementos ópticos, siendo la interacciones entre fotones más rápidas que las interacciones entre electrones, en la computación cuántica, que utiliza los fenómenos a nivel de partícula atómica para implementar puertas lógicas en una variante de la lógica multivalorada denominada lógica cuántica, la computación basada en ADN y biología molecular, que basa su implementación de puertas lógicas en interacciones químicas de proteínas y ácidos o intercambios químicos entre células, como en las sinapsis neuronales. Todo ello nos conduce a pensar que estas tecnologías, aunque no resulten productivas para los problemas de computación actuales; por ejemplo, la computación basada en elementos biológicos resulta más lenta que su equivalente electrónico pero permite el procesado paralelo en una escala mucho mayor, lo que puede ser también una propiedad de la computación cuántica según la implementación que siga; tienen una relación inherente con la base física o biológica de los seres vivos, por lo que podrían darnos esa capacidad de imitación a todos los niveles que mencionamos al principio del capítulo.

\chapter{La definición de inteligencia}

\epigraph{``[\textit{Intelligence is}] Achieving complex goals in complex environments''}{B. Goertzel, 2006}

La definición de lo que es inteligencia es naturalmente un apartado importante, aunque de difícil solución en el campo de la IA. En primer lugar, trataremos de diferenciar la propiedad ``inteligencia'' que nos parece de un platonismo peligroso al declarar que ésta existe como entidad, pero no es designable lingüísticamente ni en el universo físico, y reemplazarla por ``comportamientos inteligentes'' o ``tareas para las que es necesaria inteligencia''. Se predica normalmente que ser inteligente es un atributo humano, aunque si tomamos ciertas definiciones de inteligencia como válidas podemos atribuír inteligencia a todos los seres vivos o incluso a seres carentes de vida como piedras, que, intuitivamente no parecen tener ninguno de los atributos necesarios para la inteligencia. Es por tanto un tema que merece ser tratado con mucho cuidado.

Tal y como se dice en \cite{ciberneticsAshby}: ``La cibernética\footnote{\textit{Cibernética} era, en general, el nombre de lo que hoy referimos como Inteligencia Artificial en los años 50 y 60} es a la máquina real (electrónica, mecánica, neural o económica) lo que la geometría es a los objetos materiales de nuestro espacio terrestre'' y más adelante ``La cibernética es entonces indiferente al reproche de que algunas de las máquinas que estudia no están incluídas en las que encontramos entre nosotros''. Dejando a un lado que Ashby asume que el comportamiento inteligente es claramente mecánico, sacamos de aquí la idea de que la Inteligencia Artificial, en primer lugar, no escapa a la creación de comportamientos inteligentes fuera del ámbito humano pues tal vez no interesa la reproducción de todos los comportamientos a los que atribuiríamos inteligencia y, en segundo lugar, que comportamiento inteligente no es sólamente predicable de seres vivos, al ser la economía la ciencia que estudia las redes de intercambio. Se entiende todo sistema complejo, desde la maquinaria formada por múltiples máquinas simples hasta las redes sociales pasando por los sistemas neuronales como una máquina compleja cuyas leyes son:

\begin{enumerate}
	\item finitas y cogentes
	\item imitables por otras máquinas
	\item sólo dependientes de lo observable externamente
\end{enumerate}
 
Es decir, en el caso de Ashby aún no se había dado el giro cognitivista a respecto de los fenómenos mentales, y Ashby trata la mente como un constructo mecánico cuyas piezas son irrelevantes y de las cuales el funcionamiento observable es lo único relevante. Un funcionalismo que será, 50 años más tarde, destituído en las ciencias cognitivas por dos elementos: en la inteligencia artificial por las técnicas aplicadas a problemas reales, ya que la investigación en la explicación de la mente y el conocimiento será otorgada a los psicólogos, y en la psicología por el cognitivisimo, una corriente que identifica como clave en el comportamiento inteligente a los procesos mentales no observables directamente. Este funcionalismo se da también en la definición de \cite{Turing1950cmi}, ya que identifica comportamiento inteligente como ``comportamiento percibido como humano por un ser humano''. Creemos que esta definición es limitante, pues sólo identifica comportamientos inteligentes o pensamientos con actuaciones humanas, siendo ambas identificaciones comportamiento-pensamiento y humano-inteligente problemáticas: la identificación comportamiento-pensamiento impide a la IA dar una descripción de lo que es pensamiento más allá de ``es comportamiento'' y la identificación humano-inteligente impide la resolución de problemas que están más allá del proceso de pensamiento humano en formas diferentes que un simple aumento en la velocidad o precisión de los mismos. 

Veremos a continuación un pequeño resumen de la segunda mitad del siglo XX que explica la convergencia de diversos grupos científicos de lingüistas, psicólogos, neurocientíficos, economistas, filósofos y computólogos en lo que hoy se conoce como \textit{ciencias cognitivas}.

\section{Las ciencias cognitivas y el estudio de la inteligencia}

Definimos \textit{ciencia cognitiva} como toda ciencia que se ocupa de uno o más aspectos de los fenómenos de la cognición. Como tales, las ciencias cognitivas forman un conjunto de ciencias y al mismo tiempo un campo de investigación interdisciplinar cuyo tema central es el estudio de la cognición humana, animal y mecánica \parencite[p.20]{pmf07}, que es a la vez la fuente, se cree, de los comportamientos inteligentes. Se distinguen dos conceptos de cognición: cognición A, aquella que se refiere a la acción de tomar en cuenta una realidad o, dicho de un modo más apropiado a la terminología científica, como recepción de información y cognición B, como el uso y manejo de dicha información. Tanto en el primer y segundo caso se hacen asunciones sobre la capacidad del ser humano de recibir o hacer uso de cualquier información, así como la existencia en cierto grado de dicha información. Existen múltiples concepciones de la información que sirven de base a conocimientos tan diversos como la medición cuantitativa del contenido de un texto fuente (p.e. la concepción informativa de Shannon como desorden) o la fundación de la capacidad deductiva de la
lógica clásica (p.e. la concepción objetivista informativa de Corcoran).

Tal y como lo expresan Allen Newell y Herbert Simon en \textit{Human Problem Solving}
(1972) el ser humano y su capacidad intelectual, y al mismo tiempo las máquinas formales de cómputo (i.e. las máquinas Turing-equivalentes) pertenecen al género, por analogía biológica, `sistema de procesamiento de información'. La constitución de las ciencias cognitivas como grupo de interés en las funciones cognitivas de animales, humanos y máquinas, comienza en la llamada ``conferencia Dartmouth'', una conferencia de matemáticos y lógicos que pone de manifiesto el auge de una disciplina informática llamada inteligencia artificial, cuyo objetivo es la obtención de comportamiento que denominaríamos inteligente en sistemas artificiales: a saber, máquinas sentientes, capaces de actuación o razonamiento autónomo. Nota bene: Más allá de la cognición en animales y máquinas, se comienza a sospechar que los reinos de las plantas, los hongos e incluso los virus tienen algún tipo de cognición. Por ejemplo, se ha demostrado que las plantas segregan neurotransmisores cuando se identifica una situación de estrés (i.e.\ aumento en la acidez del suelo o carencia de agua) y mediante esos neurotransmisores se regula su propio crecimiento \footnote{\url{http://www.sciencedaily.com/releases/2015/07/150729085922.htm} }.

Otro de los momentos importantes para este grupo de interés es el nacimiento de la psicología cognitiva, manifestado en tres hechos: la fundación en 1960 del Harvard Center for Cognitive Studies, la publicación en ese mismo año de Plans and the structure of Behaviour y la publicación en 1967 del primer libro de texto de esta escuela de pensamiento psicológico. Con la psicología cognitiva se recupera el mentalismo, esto es, la existencia de una vida mental interna que es al menos parcialmente ajena a la materia y se deja de lado el conductismo y el funcionalismo, como mencionamos antes. Además, el postulado de los procesos mentales que es inherente a la psicología cognitiva, asume un cierto compromiso con el computacionalismo, la existencia de procesos que podrían ser simulados o reproducidos mediante máquinas, y acerca esta escuela a los propósitos de la inteligencia artificial.

Aunque inteligencia artificial y psicología cognitiva son nucleares a las ciencias cognitivas no son, evidentemente, las únicas disciplinas que se centran en el estudio de los mecanismos de consciencia e inteligencia que se dan en animales y máquinas. Ciertas disciplinas, como la sociología cognitiva, la pedagogía y la filosofía de la mente, entre otras, trabajan también con este rumbo aportando diferentes perspectivas y técnicas de trabajo. Se pone el énfasis en que las ciencias cognitivas asumen una naturalización materialista de sus postulados, i.e.\ no existen entidades fuera del mundo físico, por tanto la mente, el espíritu y cualquier entidad a la que pueda ser asignada la función creadora de inteligencia debe tener una manifestación física y ningún tipo de atributo no-físico. Por lo tanto, todas las disciplinas que toman parte en las construcciones de las ciencias cognitivas si no asumen esta doctrina metafísica, por así llamarla, deben compatibilizarla con ella\footnote{Es éste el caso de la psicología, cuyo estudio de los procesos mentales obvia en muchos casos los procesos fisiológicos que los desencadenan.}.


% Last pages for ToC
%-------------------------------------------------------------------------------
\newpage

\printbibliography



\end{document}

